# ðŸŽ² Chapter 3: Probability, Expected Value & Law of Large Numbers

Welcome to **Chapter 3** of *Statistical Thinking for Humanities Scholars*!
Now that weâ€™ve explored how to describe data, we shift to the world of **uncertainty and prediction**. In this chapter, we explore:

* The basics of probability
* Expected value (EV)
* Law of Large Numbers (LLN)
* Bayes' Theorem (intro level)

---

## ðŸ”¢ Section 1: Basics of Probability

**Probability** is the branch of mathematics that deals with measuring uncertainty â€” what *might* happen.

### ðŸŽ¯ Key Concepts

* **Experiment**: A repeatable process (e.g., flipping a coin)
* **Outcome**: A possible result (e.g., heads or tails)
* **Event**: One or more outcomes (e.g., getting a head)
* **Probability of an event**:

  $$
  P(E) = \frac{\text{Number of favorable outcomes}}{\text{Total number of outcomes}}
  $$

### ðŸª™ Coin Toss Example

* Flip a fair coin â†’ Outcomes: heads (H), tails (T)
* $P(H) = \frac{1}{2}$

### ðŸŽ² Dice Roll Example

* Roll a fair six-sided die: Outcomes = 1, 2, 3, 4, 5, 6
* Probability of getting a 4:

  $$
  P(4) = \frac{1}{6}
  $$

---

## ðŸ§  Section 2: The Law of Large Numbers (LLN)

The **Law of Large Numbers** says that as an experiment is repeated many times, the average result gets closer to the **expected probability**.

### ðŸ” Coin Toss Simulation

| Tosses | Heads | Tails | Ratio (H\:T) |
| ------ | ----- | ----- | ------------ |
| 10     | 6     | 4     | 1.5          |
| 100    | 52    | 48    | 1.08         |
| 1000   | 498   | 502   | 0.99         |

> ðŸ” As the number of tosses increases, the outcomes stabilize around 50:50 â€” this is the **LLN** in action!

### ðŸ“˜ Real-Life Applications

* Political polling: more people = more accurate results
* Weather forecasting: repeated patterns guide future estimates
* Agriculture: estimating long-term yield from past averages

---

## ðŸ’° Section 3: Expected Value (EV)

**Expected Value** is the average result you'd expect **in the long run**.

### ðŸ” Formula:

$$
EV = \sum (\text{Value} \times \text{Probability})
$$

### ðŸŽ² Dice Example:

* Roll a fair die:

  $$
  EV = \frac{1+2+3+4+5+6}{6} = 3.5
  $$

> âš ï¸ Youâ€™ll never roll a 3.5, but itâ€™s the **average outcome over time**.

### ðŸ’µ Lottery Example:

* Win Rs. 100 with $P = 0.01$; lose Rs. 10 with $P = 0.99$

  $$
  EV = (100 \times 0.01) + (-10 \times 0.99) = 1 - 9.9 = -8.9
  $$

> ðŸ“Œ This shows the **expected loss** â€” helps you decide whether a bet is worth it.

---

## ðŸ”„ Section 4: Bayes' Theorem (Intuitive Intro)

**Bayes' Theorem** helps update probabilities **after seeing new evidence**.

### ðŸ§ª Medical Testing Example:

* Disease is rare (1 in 1000)
* Test is 99% accurate
* You test positive. What are the chances you really have the disease?

> â— Intuition says: â€œ99%!â€ but real probability is much lower â€” because the disease is rare.

This is where **Bayesâ€™ Rule** helps combine:

* Prior probability (before the test)
* Likelihood of the test being right

Weâ€™ll explore full calculations in later chapters. For now, remember:

> **Bayesâ€™ Theorem helps update beliefs when new data appears.**

---

## ðŸ§© Think & Reflect

* Why does flipping a coin many times give more reliable results?
* How does EV help in making financial decisions?
* Can two events be highly correlated but not causally related?

---

## ðŸ“š Summary Table

| Concept                  | Meaning                                       |
| ------------------------ | --------------------------------------------- |
| **Probability**          | Chance of an event happening                  |
| **Expected Value**       | Long-run average outcome                      |
| **Law of Large Numbers** | Probabilities stabilize with many repetitions |
| **Bayes' Theorem**       | Updates probability using new evidence        |

---

## ðŸ§­ Coming Up Next:

ðŸ“ˆ **Correlation, Causation, and Scatterplots**
Explore how variables relate and what we can (and canâ€™t) conclude from patterns!
